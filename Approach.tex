Our approach starts with preparing the word problem data sets. From our data we need to extract information that will help us formulate the mathematical model from the given text. There are different ways we can do it but the major ones are using a deep learning algorithm or by using a pattern matching using regular expression for instance. 
With regex (rule based) we can use different patterns to extract different information, but this is particularly hard because it is difficult to have a single pattern or even multiple patterns that can satisfy every condition or info that we want. 
Thus, we will use deep learning and packages with different models to handle post tagging and custom NER to train a model that can be used to extract required information. 

\section{Data understanding} 
Our task is to formalize a verbal description of engineering problem into mathematical model using expressions and equations. As such we need to understand the type of text available to prepare it to the modelling stage. 
The first step is the data acquisition which was gathered from collection of problems from \parencite{budak1965collection}.
We consider problems in physical phenomenon such as heat conduction, diffusion, the propagation of electromagnetic waves in conducting media and the motion of viscous fluid. 
Budak presents a series of boundary value problems where the physical process can be described by functions of two independent variables, time and coordinates.  
The problems are of a different kind, ones of hyperbolic or parabolic with each determining the differential equation representing them. 
From visual inspection of the data we can see that a pattern of information such as inequality equations, similar wording describing a condition that is useful for the formulation of equations.

To state the boundary-value problem, corresponding to a given physical problem, means to choose a function describing the physical process, and then
\begin{itemize}
    \item derive the differential equation of this function,
    \item establish boundary conditions for it,
    \item formulate the initial conditions.
\end{itemize}

\newpage
Unfortunately finding a data set of similar problem statements is not easy possibly because the area of the problems are a little advanced or not commonly solved by the online community as compared to an algebraic word problems which is widely available on the web such as \textbf{\textit{algebra.com}}, \textbf{\textit{yahoo.answers.com}} which can be scraped into data frame for processing. Thus relaying on hard text books to get data of appropriate discrepancy and not a generically similar problems and on that basis we have 24 data sets word problems as can be seen in appendix \ref{problems}. 

\section{Data preparation} 
 As a start to the data preprocessing stage, we transformed the text data into a suitable format for the modeling. 
 \begin{figure}[bht]
    \centering
    \includegraphics[width=1\textwidth]{images/pipeline.png}
    \caption{Deep model pipeline}
    \label{fig:4 complete model procesies}
\end{figure}
\subsection{Annotation and cleaning data} 
Semantic annotation is the task of annotating or labeling various concepts or tags within text, from a predefined categories such as people, objects, or company names. Machine learning models use semantically annotated data to learn how to categorize new concepts in new texts. 
For our data we choose three labels initial condition, function and boundary condition to help train our formalization model. 

Although there are tools to be used for annotation purposes, we used a manual method since the data sets are small in number. In our we cleaned the data by removing unnecessary spaces, commas and parenthesis so it can be easier for training. This process is discribed in the appendix \ref{problems}.                                   
\subsection{Exploratory data analysis (EDA)}
\subsubsection{Word embedding} 
When dealing with natural language processing, the data is often in a text format. One way of representing the text is using a word embedding technique where individual words are represented as real values vectors in a predefined vector space. Word embedding are N-dimensional vectors that try to capture word meaning and context in their values. Each word is mapped to one vector and the vector values are learned in a way that resembles a neutral network. This is done by algorithms that cluster similar words together and projects it in multiple dimension of vectors. 
For example:
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\textwidth]{images/word embedding.PNG}
    \caption{word embedding}
    \label{fig:word rmbedding}
\end{figure}

  There are a lot of word embedding algorithms developed with Word2Vec being the most famous. GloVe and fasttext are also other algorithms in the industry. spaCy uses word2vec on its pretrained English model. Word2vec takes as its input a large corpus of text and produces a vector space with each unique word in the corpus being assigned a corresponding vector in the space.  
Last the data is splitted into training and test data by a common ratio 80/20.
\newpage
\section{Modelling} 
In NLP most tasks such as tagger, parser, text categorizer and many other functionalities are product of statistical models. To extract the information embedded in the texts we will use a technique called Named-entity recognition. 
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\textwidth]{images/spacys_NER_model.png}
    \caption{NER model of spaCy}
    \label{fig:NER}
\end{figure}
 
\subsection{Named-entity recognition (NER)}
NER is sub task of information extraction that seek to locate and classify named entities mentioned in unstructured text into predefined categories such as person names, organization,location, medical codes, time expression,quantities, monetary values, etc.  
\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1\textwidth]{images/NER_annotation.png}
    \caption{NER labeling}
    \label{fig:annotationl}
\end{figure} 
With NER we can identify the meaning of "Apple" to mean the company apple and not the fruit which can be a node in a knowledge base outside of the linguistic network. 
Similar to the other NLP tasks mentioned above, this also is done by statistical Machine leaning algorithms such as RNN, LSTM, random forest or feed forward neural network. 

\subsubsection{spaCys NER model} 
Instead of looking at the labeled text and its meaning NER looks into the surrounding words and try to figure out what the labeled word mean. spaCy uses a new deep learning concept called \textbf{\textit{embed, encode, attend, predict.}}

\begin{itemize}
    \item Embed is a process of turning a text or sparse, binary vectors into shorter, dense vectors using a bloom embedding algorithm or hash embed. This takes a different approach to the common embedding representations.
    \item Encode is a process that converts a sequence of word vectors into a sentence matrix where each row represents a meaning of each token in the context of the rest of the sentence there by capturing the semantic representation by using a convolutional neural network which are several layers of convolutions, which is a linear operation that involves the multiplication of a set of weights with inputs much like traditional neural network, with non linear activation functions applied to the outputs.
    \item Attend is a process of reducing the sentence matrix representation of the encoding output into a single vector using the attention mechanism which is a technique that mimics cognitive attention which means the network focuses its memory into important parts of the data it processes. 
    \item Predict is a mechanism of predicting given label with a simple multi layer percepton given an input.
    
\end{itemize}
With the above four principles spaCys NER can be used to extract information from our math word problems. But since the NER model english model is trained on a common conversational English we can customize the model by training it with our own corpus. 
\begin{figure}[h]
    \centering
\includegraphics[width=1\textwidth]{images/spacy_NLP_pipeline.png}
    \caption{NLP pipepline for spaCy}
    \label{fig:NLP pipelinel}
\end{figure} 
Using spaCy we will custom build the NER model and leave the other component models such as tokenizer, tagger and parser intact to get the document object from the input text. Figure \ref{fig:NLP pipelinel} helps to explain the processing pipeline.
When formulating the mathematical equations what we need from the model is semantically correct inputs or know value. Thus we have boundary conditions and initial conditions from the semantic.
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
\begin{lstlisting}[language=python]

doc=nlp("""The initial temperature of a rod 0 < x < L0 thermally 
insulated along the surface equals zero,and a constant 
temperature is maintained at its ends 
u(0,t)=U1=const ; u(L0,t)=U2=const ; 0 < t < 1: 
Find the temperature u(x; t) of the rod for t > 0.""")   

for ent in doc.ents: 
    print(ent.text,ent.start_char,ent.end_char,ent.label_)
spacy.displacy.serve(doc,style='ent')
\end{lstlisting}
\newpage
The code snippet above shows a document object which incorporates a labelled text being outputted from as seen on \ref{fig:NLP Visulaizationl} from a trained NLP object. Please refer to appendix \ref{Code} to look into the detail implementation of the NLP object. The whole project code can be found in the jupyter notebook file, which is uploaded in the GitHub repository (formulation\_math1.ipynb).

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1.0\textwidth]{images/displacy_NER_result.png}
    \caption{visualization of NER model output}
    \label{fig:NLP Visulaizationl}
\end{figure} 
From figure \ref{fig:NLP Visulaizationl} above we can see the initial condition label was missed by our model so we have to retrain it until we have the most possible fit. 
Because it might be an overdue/overkill  to use a machine learning algorithm to classify the problems, we used a basic conditional statements to overcome the classification.

For the functional equation formulation the regex can only handles patterns of the problems given and a variation or change of pattern must be added to handle new patterns and conditions. 