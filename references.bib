@inproceedings{wang2017deep,
  title={Deep neural solver for math word problems},
  author={Wang, Yan and Liu, Xiaojiang and Shi, Shuming},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={845--854},
  year={2017}
}
@article{roy2016equation,
  title={Equation parsing: Mapping sentences to grounded equations},
  author={Roy, Subhro and Upadhyay, Shyam and Roth, Dan},
  journal={arXiv preprint arXiv:1609.08824},
  year={2016}
}
@article{koncel2015parsing,
  title={Parsing algebraic word problems into equations},
  author={Koncel-Kedziorski, Rik and Hajishirzi, Hannaneh and Sabharwal, Ashish and Etzioni, Oren and Ang, Siena Dumas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={3},
  pages={585--597},
  year={2015},
  publisher={MIT Press}
}
@inproceedings{shi2015automatically,
  title={Automatically solving number word problems by semantic parsing and reasoning},
  author={Shi, Shuming and Wang, Yuehui and Lin, Chin-Yew and Liu, Xiaojiang and Rui, Yong},
  booktitle={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  pages={1132--1142},
  year={2015}
}
@inproceedings{hosseini2014learning,
  title={Learning to solve arithmetic word problems with verb categorization},
  author={Hosseini, Mohammad Javad and Hajishirzi, Hannaneh and Etzioni, Oren and Kushman, Nate},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={523--533},
  year={2014}
}
@inproceedings{kushman2014learning,
  title={Learning to automatically solve algebra word problems},
  author={Kushman, Nate and Artzi, Yoav and Zettlemoyer, Luke and Barzilay, Regina},
  booktitle={Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={271--281},
  year={2014}
}
@inproceedings{chambers2011template,
  title={Template-based information extraction without the templates},
  author={Chambers, Nathanael and Jurafsky, Dan},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies},
  pages={976--986},
  year={2011}
}
@inproceedings{chen2020mapping,
  title={Mapping natural-language problems to formal-language solutions using structured neural representations},
  author={Chen, Kezhen and Huang, Qiuyuan and Palangi, Hamid and Smolensky, Paul and Forbus, Ken and Gao, Jianfeng},
  booktitle={International Conference on Machine Learning},
  pages={1566--1575},
  year={2020},
  organization={PMLR}
}

@inproceedings{
Lample2020Deep,
title={Deep Learning For Symbolic Mathematics},
author={Guillaume Lample and Fran√ßois Charton},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1eZYeHFDS}
}

@inproceedings{huang-etal-2018-neural,
    title = "Neural Math Word Problem Solver with Reinforcement Learning",
    author = "Huang, Danqing  and
      Liu, Jing  and
      Lin, Chin-Yew  and
      Yin, Jian",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/C18-1018",
    pages = "213--223",
    abstract = "Sequence-to-sequence model has been applied to solve math word problems. The model takes math problem descriptions as input and generates equations as output. The advantage of sequence-to-sequence model requires no feature engineering and can generate equations that do not exist in training data. However, our experimental analysis reveals that this model suffers from two shortcomings: (1) generate spurious numbers; (2) generate numbers at wrong positions. In this paper, we propose incorporating copy and alignment mechanism to the sequence-to-sequence model (namely CASS) to address these shortcomings. To train our model, we apply reinforcement learning to directly optimize the solution accuracy. It overcomes the {``}train-test discrepancy{''} issue of maximum likelihood estimation, which uses the surrogate objective of maximizing equation likelihood during training while the evaluation metric is solution accuracy (non-differentiable) at test time. Furthermore, to explore the effectiveness of our neural model, we use our model output as a feature and incorporate it into the feature-based model. Experimental results show that (1) The copy and alignment mechanism is effective to address the two issues; (2) Reinforcement learning leads to better performance than maximum likelihood on this task; (3) Our neural model is complementary to the feature-based model and their combination significantly outperforms the state-of-the-art results.",
}

@article{liddy2001natural,
  title={Natural language processing},
  author={Liddy, Elizabeth D},
  year={2001}
}

@book{indurkhya2010handbook,
  title={Handbook of natural language processing},
  author={Indurkhya, Nitin and Damerau, Fred J},
  volume={2},
  year={2010},
  publisher={CRC Press}
}
@article{budak1965collection,
  title={A Collection of Problems on Mathematical Physics},
  author={Budak, BM and Samarskii, AS and Tikhonov, AN and Kahn, Peter B},
  journal={American Journal of Physics},
  volume={33},
  number={10},
  pages={862--862},
  year={1965}
}